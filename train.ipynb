{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ последствий наводнений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, rasterio, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms  import v2\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from rasterio.windows import Window\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить плитки с перекрытием\n",
    "\n",
    "def get_tiles_with_overlap(image_width: int, image_height: int, \n",
    "                           tile_size: int, overlap: int) -> List[Window]:\n",
    "    \"\"\"\n",
    "    Вычислите окна для плиток с заданным перекрытием по всему изображению\n",
    "\n",
    "    Parameters:\n",
    "        image_width (int): Ширина входного изображения в пикселях.\n",
    "        image_height (int): Высота входного изображения в пикселях.\n",
    "        tile_size (int): Размер каждой плитки (предполагается квадратная плитка).\n",
    "        overlap (int): Количество перекрывающихся пикселей между соседними плитками.\n",
    "\n",
    "    Returns:\n",
    "        List[Window]: Список объектов rasterio Window, представляющих каждую плитку.\n",
    "    \"\"\"\n",
    "    step_size = tile_size - overlap\n",
    "    tiles = []\n",
    "    for y in range(0, image_height, step_size):\n",
    "        for x in range(0, image_width, step_size):\n",
    "            window = Window(x, y, tile_size, tile_size)\n",
    "            # Отрегулируйте окно, если оно выходит за границы изображения\n",
    "            window = window.intersection(Window(0, 0, image_width, image_height))\n",
    "            tiles.append(window)\n",
    "    return tiles\n",
    "\n",
    "def save_tile(src_dataset: rasterio.io.DatasetReader, window: Window, \n",
    "              output_folder: str, tile_index: int, image_id: int) -> None:\n",
    "    \"\"\"\n",
    "    Извлечение и сохранение одной плитки из исходного набора данных.\n",
    "\n",
    "    Parameters:\n",
    "        src_dataset (rasterio.io.DatasetReader): Открытый rasterio-датасет (входное изображение).\n",
    "        window (Window): Окно (rasterio Window object) определяющее плитку.\n",
    "        output_folder (str): Папка, в которую будут сохранены плитки.\n",
    "        tile_index (int): Индекс плитки, которая будет использоваться для именования файла.\n",
    "        image_id (int): Идентификатор изображения, который будет использоваться для именования файла.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    transform = src_dataset.window_transform(window)\n",
    "    tile_data = src_dataset.read(window=window)\n",
    "    \n",
    "    profile = src_dataset.profile\n",
    "    profile.update({\n",
    "        'driver': 'GTiff',\n",
    "        'height': window.height,\n",
    "        'width': window.width,\n",
    "        'transform': transform\n",
    "    })\n",
    "    \n",
    "    output_filename = os.path.join(output_folder, f\"tile_{image_id}_{tile_index}.tif\")\n",
    "    with rasterio.open(output_filename, 'w', **profile) as dst:\n",
    "        dst.write(tile_data)\n",
    "\n",
    "def split_image(image_path: str, output_folder: str, mask_path: Optional[str] = None, \n",
    "                tile_size: int = 512, overlap: int = 128, image_id: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Разделить большое изображение GeoTIFF и соответствующую маску (если она предусмотрена) на тайлы с перекрытием и сохраните их.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Путь к файлу входного изображения TIFF.\n",
    "        mask_path (Optional[str]): Путь к соответствующей изображению маске в формате TIFF. Если None, обрабатывается только изображение.\n",
    "        output_folder (str): Папка, в которую будут сохранены плитки.\n",
    "        tile_size (int, optional): Размер плитки. По умолчанию512x512.\n",
    "        overlap (int, optional): Количество пикселей, которые должны перекрываться между плитками. По умолчанию 128 pixels.\n",
    "        image_id (int, optional): Идентификатор входного изображения, который будет использоваться для именования файла. \n",
    "            Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with rasterio.open(image_path) as src_image:\n",
    "        image_width = src_image.width\n",
    "        image_height = src_image.height\n",
    "\n",
    "        # Создайте выходные каталоги для изображений и масок (если они есть).\n",
    "        images_folder = os.path.join(output_folder, 'images')\n",
    "        os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "        if mask_path:\n",
    "            masks_folder = os.path.join(output_folder, 'masks')\n",
    "            os.makedirs(masks_folder, exist_ok=True)\n",
    "\n",
    "        # Получить список плиток с перекрытием\n",
    "        tiles = get_tiles_with_overlap(image_width, image_height, tile_size, overlap)\n",
    "\n",
    "        # Сохраните плитки изображения (и плитки маски, если они предусмотрены)\n",
    "        if mask_path:\n",
    "            with rasterio.open(mask_path) as src_mask:\n",
    "                for idx, window in tqdm(enumerate(tiles)):\n",
    "                    save_tile(src_image, window, images_folder, idx, image_id)\n",
    "                    save_tile(src_mask, window, masks_folder, idx, image_id)\n",
    "        else:\n",
    "            for idx, window in tqdm(enumerate(tiles)):\n",
    "                save_tile(src_image, window, images_folder, idx, image_id)\n",
    "\n",
    "\n",
    "output_folder = 'data/' \n",
    "\n",
    "for i, name in enumerate(['1.tif', '2.tif', '4.tif', '5.tif']):#, '6_1.tif', '9_1.tif']):\n",
    "    for over in [32, 64, 96]:\n",
    "        split_image(\n",
    "            image_path=f'train/images/{name}', mask_path=f'train/masks/{name}',\n",
    "            output_folder=output_folder, tile_size=256,\n",
    "            overlap=32, image_id=i*100+over\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение списков данных для обучения и валидации\n",
    "\n",
    "def get_data_list(img_path):\n",
    "    \"\"\"\n",
    "    Retrieves a list of file names from the given directory.\n",
    "    \"\"\"\n",
    "    name = []\n",
    "    for _, _, filenames in os.walk(img_path): # given a directory iterates over the files\n",
    "        for filename in filenames:\n",
    "            f = filename.split('.')[0]\n",
    "            name.append(f)\n",
    "\n",
    "    df =  pd.DataFrame({'id': name}, index = np.arange(0, len(name))\n",
    "                       ).sort_values('id').reset_index(drop=True)\n",
    "    df = df['id'].values\n",
    "\n",
    "    return np.delete(df, 0)\n",
    "\n",
    "gdl = get_data_list('data/images/')\n",
    "\n",
    "train_list, val_list = random_split(gdl, [.95, .05], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_len, val_len = len(train_list), len(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_padding(image, target_size=256):\n",
    "    \"\"\"\n",
    "    Pad an image to a target size using reflection padding.\n",
    "    \"\"\"\n",
    "    height, width = image.shape[1:3]\n",
    "    pad_height = max(0, target_size - height)\n",
    "    pad_width = max(0, target_size - width)\n",
    "    padded_image = np.pad(image, ((0, 0), (0, pad_height),\n",
    "                                  (0, pad_width)), mode='reflect')\n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def mask_padding(mask, target_size=256):\n",
    "    \"\"\"\n",
    "    Pad a mask to a target size using reflection padding.\n",
    "    \"\"\"\n",
    "    height, width = mask.shape\n",
    "    pad_height = max(0, target_size - height)\n",
    "    pad_width = max(0, target_size - width)\n",
    "    padded_mask = np.pad(mask, ((0, pad_height), (0, pad_width)),\n",
    "                         mode='reflect')\n",
    "    return padded_mask\n",
    "\n",
    "augs = v2.Compose([v2.RandomHorizontalFlip(.4), \n",
    "                   v2.RandomVerticalFlip(.3),\n",
    "                   v2.RandomRotation(degrees=90),\n",
    "                   ])\n",
    "\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, file_names, aug=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.file_names = file_names\n",
    "        self.aug = aug\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with rasterio.open(self.img_path + self.file_names[idx] + '.tif') as fin:\n",
    "            image = fin.read()\n",
    "        # Добавим нормализацию\n",
    "        image = v2.Normalize(mean=[0.]*10, std=[1.]*10)(image)\n",
    "        image = image_padding(image).astype(np.float32)\n",
    "\n",
    "        with rasterio.open(self.mask_path + self.file_names[idx] + '.tif') as fin:\n",
    "            mask = fin.read(1)\n",
    "        mask = mask_padding(mask)\n",
    "\n",
    "        # Добавление расширения данных\n",
    "        if self.aug:\n",
    "            image, mask = augs(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "train_ds = WaterDataset(img_path='data/images/', mask_path='data/masks/',\n",
    "                        file_names=train_list, aug=True\n",
    "                        )\n",
    "\n",
    "val_ds = WaterDataset(img_path='data/images/', mask_path='data/masks/',\n",
    "                      file_names=val_list)\n",
    "\n",
    "bs = 32\n",
    "train = DataLoader(train_ds, batch_size = bs, shuffle = True)\n",
    "val = DataLoader(val_ds, batch_size = bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор модели и параметров обучения\n",
    "\n",
    "model = smp.DeepLabV3Plus(encoder_name=\"timm-regnety_064\", in_channels=10, classes=2)\n",
    "\n",
    "model.segmentation_head[0] = torch.nn.Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "model.segmentation_head[2] = torch.nn.Sigmoid()\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "lr, dt, l1, l0, ne = 9e-4, .99, 1e9, 1e9, 25\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха № 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [12:58<00:00,  2.61s/it]\n",
      "100%|██████████| 16/16 [00:31<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000672455 \t train: 0.00025956, 0.02839847\t val: 0.00001182, 0.02899331\n",
      "Эпоха № 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [12:52<00:00,  2.59s/it]\n",
      "100%|██████████| 16/16 [00:33<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000502440 \t train: 0.00025428, 0.02847202\t val: 0.00001145, 0.02898498\n",
      "Эпоха № 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [13:10<00:00,  2.65s/it]\n",
      "100%|██████████| 16/16 [00:31<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000375409 \t train: 0.00022379, 0.02861754\t val: 0.00001057, 0.02925439\n",
      "Эпоха № 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [13:07<00:00,  2.64s/it]\n",
      "100%|██████████| 16/16 [00:33<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000280495 \t train: 0.00020763, 0.02867014\t val: 0.00000969, 0.02934580\n",
      "Эпоха № 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [13:08<00:00,  2.65s/it]\n",
      "100%|██████████| 16/16 [00:32<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000209578 \t train: 0.00019564, 0.02888609\t val: 0.00000945, 0.02933031\n",
      "Эпоха № 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [13:06<00:00,  2.64s/it]\n",
      "100%|██████████| 16/16 [00:31<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000156591 \t train: 0.00018547, 0.02895013\t val: 0.00000941, 0.02952085\n",
      "Эпоха № 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [13:52<00:00,  2.80s/it]\n",
      "100%|██████████| 16/16 [00:32<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000117000 \t train: 0.00018043, 0.02893837\t val: 0.00000880, 0.02945227\n",
      "Эпоха № 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [14:19<00:00,  2.89s/it]\n",
      "100%|██████████| 16/16 [00:36<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000087419 \t train: 0.00017274, 0.02910861\t val: 0.00000869, 0.02958956\n",
      "Эпоха № 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 267/298 [11:52<01:23,  2.70s/it]"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "\n",
    "for epoch in range(ne):\n",
    "    print('Эпоха №', epoch + 1)\n",
    "    model.train()\n",
    "    tr_loss, tr_f1 = 0, 0\n",
    "    for i, batch in enumerate(tqdm(train)):\n",
    "        ims, gts = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "        preds = model(ims)\n",
    "        loss = torch.nn.functional.binary_cross_entropy(preds, gts.unsqueeze(1).to(dtype=torch.float32))\n",
    "        tr_loss += loss.item()\n",
    "        tr_f1 += f1_score(torch.round(preds.view(-1)).int().to('cpu'),\n",
    "                          gts.view(-1).to('cpu'), average='macro')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if (i+1)%10 == 0:\n",
    "            lr *= dt\n",
    "            optimizer.param_groups[0][\"lr\"] = lr\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_f1 = 0, 0\n",
    "        for ims, gts in tqdm (val):\n",
    "            ims, gts = ims.to('cuda'), gts.to('cuda')\n",
    "            preds = model(ims)\n",
    "            val_loss += torch.nn.functional.binary_cross_entropy(preds, \n",
    "                                                                 gts.unsqueeze(1).to(dtype=torch.float32))\n",
    "            val_f1 += f1_score(torch.round(preds.view(-1)).int().to('cpu'),\n",
    "                               gts.view(-1).to('cpu'), average='macro')\n",
    "    tr_loss /= tr_len\n",
    "    tr_f1 /= tr_len\n",
    "    val_loss /= tr_len\n",
    "    val_f1 /= val_len\n",
    "    print(f'lr: {lr:.9f} \\t train: {tr_loss:.8f}, {tr_f1:.8f}\\t val: {val_loss:.8f}, {val_f1:.8f}')\n",
    "    if val_loss > l0 and l1 > l0: break\n",
    "    elif epoch > 1 and val_loss < l1 < l0:\n",
    "        torch.save(model, f\"best_model_0.pt\")\n",
    "    l1, l0 = loss, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986936666625109"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оценка сегментации\n",
    "\n",
    "def mIoU(pred, gt):\n",
    "    with torch.no_grad():\n",
    "        pred, gt = pred.contiguous().view(-1), gt.contiguous().view(-1)\n",
    "        iou_per_class = []\n",
    "        for c in range(2):\n",
    "            match_pred = pred == c\n",
    "            match_gt   = gt == c\n",
    "            if match_gt.long().sum().item() == 0:\n",
    "                # iou_per_class.append(-1)\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(match_pred, match_gt).sum().float().item()\n",
    "                union = torch.logical_or(match_pred, match_gt).sum().float().item()\n",
    "                iou = (intersect + 1e-10) / (union + 1e-10)\n",
    "                iou_per_class.append(iou)        \n",
    "    return np.nanmean(iou_per_class)\n",
    "\n",
    "mIoU(torch.round(model(ims)).int(), gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ последствий наводнений\n",
    "\n",
    "**Описание датасета**\n",
    "\n",
    "папка train:\n",
    "- images мультиспектральные снимки большие\n",
    "- masks соотвествующие им бинарные маски\n",
    "- example_preds это пример предсказаний, чтобы можно было метрики прогнать\n",
    "- osm карта OpenStreetMap для кусочка на котором будет считаться бизнес метрика\n",
    "\n",
    "дальше подсчет метрик:\n",
    "- calculate_metrics.py - принимает пути до файлов и считает много раз взвешенное\n",
    "1. на вход получает две папки с предиктами и масками и сранивает macro_f1_score\n",
    "2. на вход получает маску и предикт для двух снимков до и после затопления, а также osm для этой территории - на основе этих данных, вычисляется количество затопленных домов. Далее вычисляется macro_f1_score для ситуации до и отдельно для ситуации после, а результаты усредняются.\n",
    "3. Усредняется результат первого и второго пункта\n",
    "- run_calculation.sh - скрипт для запуска py файла, как пример\n",
    "\n",
    "helper.py - много разных вспомогательных функций. \n",
    "- Визуализация\n",
    "- Деление большого снимка на тайлы, чтобы собрать датасет нужного патча\n",
    "- Даталодер с паддингами для мультиспектральных данных на pytorch\n",
    "- Примерный пайплайн, как можно сохранить результаты работы модели\n",
    "\n",
    "\n",
    "**Предлагается разработать модель для сегментации водной поверхности на основе мультиспектральных данных, а также провести количественную оценку затопленных объектов инфраструктуры.**\n",
    "\n",
    "***На выходе нужно, чтобы `pipline` прочитал большое изображение, разделил его на части, сделал вывод масок, а затем объединил результат в большую маску и сохранил в geoTiff***\n",
    "\n",
    "**У нас будут Sentinel-2A снимки с 10 каналами:**\n",
    "\n",
    "| Name | Description                                          | Resolution |\n",
    "|------|------------------------------------------------------|------------|\n",
    "| B02  | Blue, 492.4 nm (S2A), 492.1 nm (S2B)                 | 10m        |\n",
    "| B03  | Green, 559.8 nm (S2A), 559.0 nm (S2B)                | 10m        |\n",
    "| B04  | Red, 664.6 nm (S2A), 665.0 nm (S2B)                  | 10m        |\n",
    "| B05  | Vegetation red edge, 704.1 nm (S2A), 703.8 nm (S2B)  | 20m        |\n",
    "| B06  | Vegetation red edge, 740.5 nm (S2A), 739.1 nm (S2B)  | 20m        |\n",
    "| B07  | Vegetation red edge, 782.8 nm (S2A), 779.7 nm (S2B)  | 20m        |\n",
    "| B08  | NIR, 832.8 nm (S2A), 833.0 nm (S2B)                  | 10m        | \n",
    "| B8A  | Narrow NIR, 864.7 nm (S2A), 864.0 nm (S2B)           | 20m        |\n",
    "| B11  | SWIR, 1613.7 nm (S2A), 1610.4 nm (S2B)               | 20m        |\n",
    "| B12  | SWIR, 2202.4 nm (S2A), 2185.7 nm (S2B)               | 20m        |\n",
    "\n",
    "**И маски от 0 до 1:**\n",
    "\n",
    "0. backgound\n",
    "1. water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача заключалось в разработке модели для сегментации водной поверхности - получить изображение, где вода - белая область, остальная поверхность - черная.  Решение проводилось на основе исследования мультиспектральных данных - спутниковых снимков по 10 каналам: видимого диапазона, индексов растительности и разных областей инфракрасного диапазона. Количественная оценка затопленных объектов инфраструктуры проводиться на основе полученного черно-белого изображения - маски. Полученное решение должно прочитать большое изображение, разделил его на части, сделал вывод масок, а затем объединил результат в большую маску, сохранил ее в формате geoTiff.\n",
    "Особенностью полученного решения является использование глубоких нейронных сетей: \n",
    "- современные архитектура и кодировщик изображений\n",
    "- адаптация исходной архитектуры нейронной сети под цели задачи: 10 входных каналов и 1 выходной\n",
    ". Особенности обучения модели:\n",
    "- разделение исходных данных (10-ти канальных \"изображений\") на части с различным перекрытием;\n",
    "- дальнейшее расширение набора обучаемых данных на основе отражений и поворотов;\n",
    "- изменение шага обучения модели\n",
    "- контроль переобучения модели"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
